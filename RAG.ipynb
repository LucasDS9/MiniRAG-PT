{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1ccd9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "017a3308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num√©ro de p√°ginas : 4\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"Pensamento_maquina.pdf\"\n",
    "\n",
    "reader = PdfReader(pdf_path)\n",
    "\n",
    "print(f\"Num√©ro de p√°ginas : {len(reader.pages)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f757d8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho total do texto: 9049\n"
     ]
    }
   ],
   "source": [
    "full_text = \"\"\n",
    "\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    full_text += text + \"\\n\"\n",
    "\n",
    "print(\"Tamanho total do texto:\", len(full_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "67412f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero total de chunks: 24\n",
      "\n",
      "====================\n",
      "CHUNK 0\n",
      "====================\n",
      "O Pensamento Humano e o Pensamento das \n",
      "M√°quinas\n",
      "Introdu√ß√£o\n",
      "Desde os prim√≥rdios da Ô¨ÅlosoÔ¨Åa, o ser humano busca compreender a pr√≥pria mente. \n",
      "Perguntas como o que √© pensar?, como surge o conhecimento? e o que nos torna conscientes? \n",
      "atravessam s√©culos de reÔ¨Çex√£o Ô¨Ålos√≥Ô¨Åca, cient√≠Ô¨Åca e cultural. Com o avan√ßo da \n",
      "tecnologia e, especialmente, com o surgimento da Intelig√™ncia ArtiÔ¨Åcial, essas quest√µes \n",
      "ganharam uma nova dimens√£o: ao criar m√°quinas capazes de executar tarefas cognitivas,\n",
      "\n",
      "====================\n",
      "CHUNK 1\n",
      "====================\n",
      "ganharam uma nova dimens√£o: ao criar m√°quinas capazes de executar tarefas cognitivas, \n",
      "o ser humano passou a se perguntar se estaria, de alguma forma, reproduzindo o pr√≥prio \n",
      "pensamento.\n",
      "A rela√ß√£o entre o pensamento humano e o pensamento das m√°quinas n√£o √© apenas \n",
      "t√©cnica. Trata-se de uma quest√£o conceitual, Ô¨Ålos√≥Ô¨Åca e √©tica. Comparar esses dois tipos \n",
      "de ‚Äúpensamento‚Äù nos obriga a reÔ¨Çetir sobre os limites da tecnologia e, ao mesmo tempo, \n",
      "sobre a natureza da mente humana.\n",
      "\n",
      "====================\n",
      "CHUNK 2\n",
      "====================\n",
      "sobre a natureza da mente humana.\n",
      "Este texto prop√µe uma an√°lise aprofundada dessa rela√ß√£o, explorando o que caracteriza o \n",
      "pensamento humano, como funciona o chamado pensamento da m√°quina, onde eles se \n",
      "aproximam, onde se distanciam e quais s√£o as implica√ß√µes dessa aproxima√ß√£o crescente.\n",
      "A natureza do pensamento humano\n",
      "O pensamento humano √© um fen√¥meno extremamente complexo. Ele n√£o pode ser \n",
      "reduzido a uma sequ√™ncia linear de opera√ß√µes l√≥gicas, pois envolve m√∫ltiplas dimens√µes\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,     \n",
    "    chunk_overlap=100,  \n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "print(\"N√∫mero total de chunks:\", len(chunks))\n",
    "\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(\"\\n====================\")\n",
    "    print(f\"CHUNK {i}\")\n",
    "    print(\"====================\")\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c1dd24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 103/103 [00:00<00:00, 407.33it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando embeddings dos chunks...\n",
      "\n",
      "N√∫mero de embeddings: 24\n",
      "Shape de UM embedding: (384,)\n",
      "\n",
      "Primeiros 10 n√∫meros do embedding do CHUNK 0:\n",
      "[ 0.02200048  0.11847524 -0.00923043  0.01974406 -0.07060358  0.00535173\n",
      "  0.03218772  0.0715929   0.0200606   0.07255962]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Gerando embeddings dos chunks...\\n\")\n",
    "\n",
    "chunk_embeddings = model.encode(chunks)\n",
    "\n",
    "print(\"N√∫mero de embeddings:\", len(chunk_embeddings))\n",
    "print(\"Shape de UM embedding:\", chunk_embeddings[0].shape)\n",
    "\n",
    "print(\"\\nPrimeiros 10 n√∫meros do embedding do CHUNK 0:\")\n",
    "print(chunk_embeddings[0][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c4196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection criada.\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.Client()\n",
    "\n",
    "collection = client.create_collection(name=\"rag_test\")\n",
    "\n",
    "print(\"Collection criada.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "207ef51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "QUERY:\n",
      "Diferen√ßa entre o pensamento humano e o pensamento da m√°quina\n",
      "\n",
      "Similaridade com cada chunk:\n",
      "\n",
      "Chunk 0: 0.8478\n",
      "Chunk 1: 0.8614\n",
      "Chunk 2: 0.7475\n",
      "Chunk 3: 0.6331\n",
      "Chunk 4: 0.6360\n",
      "Chunk 5: 0.6343\n",
      "Chunk 6: 0.5915\n",
      "Chunk 7: 0.8080\n",
      "Chunk 8: 0.7095\n",
      "Chunk 9: 0.7348\n",
      "Chunk 10: 0.7665\n",
      "Chunk 11: 0.4752\n",
      "Chunk 12: 0.6275\n",
      "Chunk 13: 0.8131\n",
      "Chunk 14: 0.7008\n",
      "Chunk 15: 0.8631\n",
      "Chunk 16: 0.8645\n",
      "Chunk 17: 0.8405\n",
      "Chunk 18: 0.6097\n",
      "Chunk 19: 0.7525\n",
      "Chunk 20: 0.7566\n",
      "Chunk 21: 0.7933\n",
      "Chunk 22: 0.8519\n",
      "Chunk 23: 0.8202\n"
     ]
    }
   ],
   "source": [
    "query = \"Diferen√ßa entre o pensamento humano e o pensamento da m√°quina\"\n",
    "\n",
    "print(\"\\nQUERY:\")\n",
    "print(query)\n",
    "\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
    "\n",
    "print(\"\\nSimilaridade com cada chunk:\\n\")\n",
    "\n",
    "for i, score in enumerate(similarities):\n",
    "    print(f\"Chunk {i}: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23243489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RANKING DOS CHUNKS MAIS RELEVANTES:\n",
      "\n",
      "\n",
      "Chunk 16 | Similaridade: 0.8645\n",
      "aus√™ncia de consci√™ncia e signiÔ¨Åcado por parte das m√°quinas. O ser humano pensa a \n",
      "partir de um ponto de vista interno, enquanto a m√°quina opera externamente sobre \n",
      "dados.\n",
      "Humanos atribuem sentido √†s informa√ß√µes com base em valores, experi√™ncias e emo√ß√µes. \n",
      "M√°quinas n√£o possuem qualquer rela√ß√£o existencial com o que processam. Elas n√£o \n",
      "sabem que existem, n√£o t√™m medo de errar, nem desejo de acert ...\n",
      "\n",
      "Chunk 15 | Similaridade: 0.8631\n",
      "reconhecimento de regularidades, algo que as m√°quinas fazem de forma extremamente \n",
      "eÔ¨Åciente.\n",
      "Essas semelhan√ßas funcionais s√£o suÔ¨Åcientes para criar a impress√£o de que as m√°quinas \n",
      "est√£o ‚Äúpensando‚Äù, mesmo que o processo subjacente seja radicalmente diferente.\n",
      "Diferen√ßas fundamentais e limites da analogia\n",
      "A principal diferen√ßa entre o pensamento humano e o pensamento da m√°quina est√° na \n",
      "aus√™ncia de  ...\n",
      "\n",
      "Chunk 1 | Similaridade: 0.8614\n",
      "ganharam uma nova dimens√£o: ao criar m√°quinas capazes de executar tarefas cognitivas, \n",
      "o ser humano passou a se perguntar se estaria, de alguma forma, reproduzindo o pr√≥prio \n",
      "pensamento.\n",
      "A rela√ß√£o entre o pensamento humano e o pensamento das m√°quinas n√£o √© apenas \n",
      "t√©cnica. Trata-se de uma quest√£o conceitual, Ô¨Ålos√≥Ô¨Åca e √©tica. Comparar esses dois tipos \n",
      "de ‚Äúpensamento‚Äù nos obriga a reÔ¨Çetir sobre os ...\n"
     ]
    }
   ],
   "source": [
    "ranking = np.argsort(similarities)[::-1]\n",
    "\n",
    "print(\"\\nRANKING DOS CHUNKS MAIS RELEVANTES:\\n\")\n",
    "\n",
    "for i in ranking[:3]:\n",
    "    print(f\"\\nChunk {i} | Similaridade: {similarities[i]:.4f}\")\n",
    "    print(f\"{chunks[i][:400]} ...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0aea8906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks inseridos no ChromaDB.\n",
      "Total armazenado: 24\n"
     ]
    }
   ],
   "source": [
    "ids = [f\"chunk_{i}\" for i in range(len(chunks))]\n",
    "\n",
    "collection.add(\n",
    "    documents=chunks,\n",
    "    embeddings=chunk_embeddings.tolist(),\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(\"Chunks inseridos no ChromaDB.\")\n",
    "print(\"Total armazenado:\", len(ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90767f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lucas\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\lucas\\.cache\\huggingface\\hub\\models--Qwen--Qwen2-0.5B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 290/290 [00:00<00:00, 373.87it/s, Materializing param=model.norm.weight]                              \n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "\n",
    "tokenizer_llm = AutoTokenizer.from_pretrained(model_name)\n",
    "model_llm = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fcd5d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt):\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Responda sempre em portugu√™s e seja objetivo.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    text = tokenizer_llm.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer_llm(\n",
    "        text,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    outputs = model_llm.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    response = tokenizer_llm.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    return response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "368a38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_rag(query, n_results=3):\n",
    "\n",
    "    query_embedding = model.encode([query]).tolist()\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=n_results\n",
    "    )\n",
    "\n",
    "    retrieved_chunks = results[\"documents\"][0]\n",
    "\n",
    "    context = \"\\n\".join(retrieved_chunks[:2])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Use apenas o contexto para responder.\n",
    "\n",
    "Contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "    answer = generate_answer(prompt)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f8410583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1. Desempenho: O c√©rebro humano, que √© o maior sistema nervoso do corpo humano, √© capaz de realizar uma variedade de tarefas complexas e precisas, como aprender novos conhecimentos, tomar decis√µes, lembrar-se de informa√ß√µes, responder a perguntas e muitos outros. Em contraste, as m√°quinas podem ser programadas para realizar tarefas mais simples e espec√≠ficas, mas n√£o t√™m o mesmo n√≠vel de habilidade cognitiva como o c√©rebro humano.\\n\\n2. Recursos de aprendizado: Os processadores humanos s√£o capazes de aprender com base no feedback, enquanto as m√°quinas n√£o possuem essa capacidade. Isso significa que'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_answer(\"Me diga 3 principais diferen√ßas entre o c√©rebro humano e o processamento das m√°quinas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrair_json(texto):\n",
    "    match = re.search(r\"\\{.*\\}\", texto, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            return json.loads(match.group())\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def avaliar_relevancia(pergunta, resposta):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Avalie de 0 a 10 o quanto a resposta responde corretamente a pergunta.\n",
    "\n",
    "Pergunta:\n",
    "{pergunta}\n",
    "\n",
    "Resposta:\n",
    "{resposta}\n",
    "\n",
    "Retorne apenas JSON:\n",
    "{{\"relevancia\": numero}}\n",
    "\"\"\"\n",
    "\n",
    "    result = generate_answer(prompt)\n",
    "    return extrair_json(result)\n",
    "\n",
    "\n",
    "\n",
    "def avaliar_fidelidade(contexto, resposta):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Avalie de 0 a 10 se a resposta usa apenas informa√ß√µes do contexto.\n",
    "Se inventar coisas, reduza a nota.\n",
    "\n",
    "Contexto:\n",
    "{contexto}\n",
    "\n",
    "Resposta:\n",
    "{resposta}\n",
    "\n",
    "Retorne apenas JSON:\n",
    "{{\"fidelidade\": numero}}\n",
    "\"\"\"\n",
    "\n",
    "    result = generate_answer(prompt)\n",
    "    return extrair_json(result)\n",
    "\n",
    "\n",
    "def avaliar_correcao(pergunta, resposta):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Avalie de 0 a 10 se a resposta est√° factualmente correta\n",
    "com base em conhecimento geral.\n",
    "\n",
    "Pergunta:\n",
    "{pergunta}\n",
    "\n",
    "Resposta:\n",
    "{resposta}\n",
    "\n",
    "Retorne apenas JSON:\n",
    "{{\"correcao\": numero}}\n",
    "\"\"\"\n",
    "\n",
    "    result = generate_answer(prompt)\n",
    "    return extrair_json(result)\n",
    "\n",
    "\n",
    "\n",
    "def avaliar_completude(pergunta, resposta):\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Avalie de 0 a 10 se a resposta est√° completa\n",
    "ou se faltam partes importantes.\n",
    "\n",
    "Pergunta:\n",
    "{pergunta}\n",
    "\n",
    "Resposta:\n",
    "{resposta}\n",
    "\n",
    "Retorne apenas JSON:\n",
    "{{\"completude\": numero}}\n",
    "\"\"\"\n",
    "\n",
    "    result = generate_answer(prompt)\n",
    "    return extrair_json(result)\n",
    "\n",
    "\n",
    "def avaliar_resposta_rag(pergunta, contexto, resposta):\n",
    "\n",
    "    return {\n",
    "        \"relevancia\": avaliar_relevancia(pergunta, resposta),\n",
    "        \"fidelidade\": avaliar_fidelidade(contexto, resposta),\n",
    "        \"correcao\": avaliar_correcao(pergunta, resposta),\n",
    "        \"completude\": avaliar_completude(pergunta, resposta)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "30ffbfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relevancia': {'relevancia': 7}, 'fidelidade': {'fidelidade': 9}, 'correcao': {'correcao': 9}, 'completude': {'completude': 9}}\n"
     ]
    }
   ],
   "source": [
    "resultado = mini_rag(\"Explique redes neurais\")\n",
    "\n",
    "avaliacao = avaliar_resposta_rag(\n",
    "    pergunta=resultado[\"query\"],\n",
    "    contexto=resultado[\"context\"],\n",
    "    resposta=resultado[\"answer\"]\n",
    ")\n",
    "\n",
    "print(avaliacao)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908a36db",
   "metadata": {},
   "source": [
    "üëâ Context Precision\n",
    "\n",
    "quanto do contexto usado era relevante\n",
    "\n",
    "üëâ Context Recall\n",
    "\n",
    "o modelo usou o que deveria usar?\n",
    "\n",
    "üëâ Faithfulness Score\n",
    "\n",
    "resposta baseada no contexto?\n",
    "\n",
    "üëâ Answer Correctness\n",
    "\n",
    "resposta est√° certa?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
